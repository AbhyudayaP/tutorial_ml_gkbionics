{
 "metadata": {
  "name": "pca"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Generating Data\n",
      "===============\n",
      "\n",
      "We first create 200 random two-dimensional data points.\n",
      "The data points are sampled from a multinomial normal distribution.\n",
      "\n",
      "You don't have to understand precisely how we do this. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "Cov = np.array([[2.9, -2.2], [-2.2, 6.5]])\n",
      "Cov"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Let's have a look at the raw data first..."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "np.set_printoptions(4, suppress=True) # show only four decimals\n",
      "print(D[:10,:]) # print the first 10 rows of D (from 0 to 9)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Do you see a relationship between the two columns? Tricky, I'd say. However, since the data is two-dimensional, we can plot the data, which allows us to see the relationship."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "plt.figure(figsize=(4,4))\n",
      "plt.scatter(D[:,0], D[:,1])\n",
      "plt.axis('equal') # equal scaling on both axis;"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We can also have a look at the covariance matrix:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print np.cov(D,rowvar=False)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Running PCA\n",
      "===========\n",
      "\n",
      "We would now like to analyze the directions in which the data varies most. For that, we \n",
      "\n",
      "1. place the point cloud in the center (0,0) and\n",
      "2. rotate it, such that the direction with most variance is parallel to the x-axis.\n",
      "\n",
      "Both steps can be done using PCA, which is conveniently available in sklearn.\n",
      "\n",
      "We start by loading the PCA class from the sklearn package and creating an instance of the class:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.decomposition import PCA\n",
      "pca = PCA()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now, `pca` is an object which has a function `pca.fit_transform(x)` which performs both steps from above to its argument `x`, and returns the centered and rotated version of `x`."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "E = pca.fit_transform(D)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "pca.components_"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "pca.mean_"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "plt.figure(figsize=(4,4))\n",
      "plt.scatter(E[:,0], E[:,1])\n",
      "plt.axis('equal');"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The covariances between different axes should be zero now. We can double-check by having a look at the non-diagonal entries of the covariance matrix:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print np.cov(E, rowvar=False)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "High-Dimensional Data\n",
      "=====================\n",
      "\n",
      "Our small example above was very easy, since we could get insight into the data by simply plotting it. This approach will not work once you have more than 3 dimensions, let's say we have the same data, but it is represented in four dimensions:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "HD = np.dot(D,np.random.uniform(0.2,3,(2,4))*(np.random.randint(0,2,(2,4))*2-1))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Lets look at the data again. First, the raw data:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print HD[:10]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "That one is more tricky. See anything? We can also try plot a few two-dimensional projections:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "plt.figure(figsize=(8,8))\n",
      "for i in xrange(4):\n",
      "    for j in xrange(4):\n",
      "        plt.subplot(4, 4, i * 4 + j + 1)\n",
      "        plt.scatter(HD[:,i], HD[:,j])\n",
      "        plt.axis('equal')\n",
      "        plt.gca().set_aspect('equal')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "HE = pca.fit_transform(HD)\n",
      "print HE[:10,:]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Here it is easy to see, that the data is **still only two-dimensional**. Let's plot the two dimensions."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "plt.figure(figsize=(4,4))\n",
      "plt.scatter(E[:,0], E[:,1])\n",
      "plt.axis('equal')\n",
      "plt.gca().set_aspect('equal')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Why does the result look differently than the two-dimensional data from which we generated it?"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "plt.figure(figsize=(8,8))\n",
      "for i in xrange(4):\n",
      "    for j in xrange(4):\n",
      "        plt.subplot(4, 4, i * 4 + j + 1)\n",
      "        plt.scatter(HE[:,i], HE[:,j])\n",
      "        plt.gca().set_xlim(-40,40)\n",
      "        plt.gca().set_ylim(-40,40)\n",
      "        plt.axis('equal')\n",
      "        plt.gca().set_aspect('equal')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Dimension Reduction with PCA\n",
      "============================\n",
      "\n",
      "We can see that there are actually only two dimensions in the dataset. \n",
      "\n",
      "Let's throw away even more data -- the second dimension -- and reconstruct the original data in `D`."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "pca = PCA(1) # only keep one dimension!\n",
      "E = pca.fit_transform(HD)\n",
      "print E[:10,:]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now lets plot the reconstructed data and compare to the original data D. We plot the original data in red, and the reconstruction with only one dimension in blue:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "D_reconstructed = pca.inverse_transform(E)\n",
      "plt.figure(figsize=(8,8))\n",
      "for i in xrange(4):\n",
      "    for j in xrange(4):\n",
      "        plt.subplot(4, 4, i * 4 + j + 1)\n",
      "        plt.scatter(HD[:,i], HD[:,j],c='r')\n",
      "        plt.scatter(D_reconstructed[:,i], D_reconstructed[:,j],c='b')\n",
      "        plt.axis('equal')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "MNIST?"
     ]
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Playing around with this Notebook"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "- What happens, when you multiply one of the data axis with a large (or small) number? \n",
      "\n",
      "  e.g. using X[:,0] *= 100\n",
      "\n",
      "  Does the result stay the same? Why/why not?\n",
      "\n",
      "-----\n",
      "\n",
      "- Try to explore the iris dataset below using PCA. \n",
      "\n",
      "  What happens if you visualize the *last* components of PCA instead of the first ones?\n",
      "\n",
      "-----\n",
      "\n",
      "- Use PCA with 1, 2 or 3 axes and reconstruct the Iris data from each. How do the results change? Show plots!"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn import datasets\n",
      "_,data,target,_,_ = datasets.load_iris().values()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}