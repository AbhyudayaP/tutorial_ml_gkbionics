{
 "metadata": {
  "name": "Logistic Regression"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Logistic Regression\n",
      "==================="
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "First, we generate some data to train to train with."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.datasets import make_blobs\n",
      "X, y = make_blobs(centers=2)   # generate dataset consisting of two Gaussian clusters"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Take a look at how large the data is and what the labels look like:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print \"X.shape:\", X.shape \n",
      "print \"y: \", y "
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "As the data is two-dimensional, we can easily visualize it:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "plt.prism()\n",
      "plt.scatter(X[:, 0], X[:, 1], c=y)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Import logistic regression from scikit-learn and generate a classification object."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.linear_model import LogisticRegression\n",
      "logreg = LogisticRegression()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Split the dataset into a training set and a test set."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "X_train = X[:50]\n",
      "y_train = y[:50]\n",
      "X_test = X[50:]\n",
      "y_test = y[50:]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "To get an idea of how hard the task is, let us visualize the data again, this time only labeling the training points.\n",
      "\n",
      "The test points are plottet as white triangles.\n",
      "Can we see which class the test points belong to?"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "plt.prism()\n",
      "plt.scatter(X_train[:, 0], X_train[:, 1], c=y_train)\n",
      "plt.scatter(X_test[:, 0], X_test[:, 1], c='white', marker='^')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "That doesn't look too hard.\n",
      "\n",
      "Now let's fit the logistic regression model to the training data, to see if we can do it automatically:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "logreg.fit(X_train, y_train)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "First, we have a look at how logistic regression did on the training set.\n",
      "\n",
      "We do this by now setting colors using the predicted class."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from utility import plot_decision_boundary\n",
      "y_pred_train = logreg.predict(X_train)\n",
      "plt.scatter(X_train[:, 0], X_train[:, 1], c=y_pred_train)\n",
      "plot_decision_boundary(logreg, X)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "That looks pretty good. It is not always that easy to inspect the result visually,\n",
      "so we can also use the logistic regression object to calculate a core for us:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print \"Accuracy on training set:\", logreg.score(X_train, y_train)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "1.0 means an accuracy of 100%, just as it looked like.\n",
      "\n",
      "Now let us have a look at how the algorithm generalizes to the test data."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "y_pred_test = logreg.predict(X_test)\n",
      "plt.scatter(X_test[:, 0], X_test[:, 1], c=y_pred_test, marker='^')\n",
      "plt.scatter(X_train[:, 0], X_train[:, 1], c=y_pred_train)\n",
      "plot_decision_boundary(logreg, X)\n",
      "print \"Accuracy on test set:\", logreg.score(X_test, y_test)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We can also take a look at the coefficients, but they probably don't tell you much."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "logreg.coef_"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "MNIST DIGITS\n",
      "-------------\n",
      "Let's go back to the MNIST example of before.\n",
      "This dataset is a bit more exciting since it is much higher dimensional and not as easy to separate.\n",
      "First we load the data again"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.datasets import fetch_mldata\n",
      "mnist = fetch_mldata(\"MNIST original\")\n",
      "X_digits, y_digits = mnist.data, mnist.target"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Remember what the dataset looks like:\n",
      "\n",
      "- 70000 examples, 784 dimensional\n",
      "- ten classes, 0-9\n",
      "- each 784 vector is a 28 * 28 digit"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print \"X_digits.shape:\", X_digits.shape\n",
      "print \"Unique entries of y_digits:\", np.unique(y_digits)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print(y_digits[0])\n",
      "plt.rc(\"image\", cmap=\"binary\")\n",
      "plt.matshow(X_digits[0].reshape(28, 28))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We could try to learn all ten classes with logistic regression, but let us keep with two classes for the moment.\n",
      "We will use two classes that are quite hard to distinguish: seven and nine.\n",
      "\n",
      "To create a dataset only consisting of the classes seven and nine, we need a new numpy trick!\n",
      "\n",
      "We can not only slice our data using ranges, like ``X[5:10]``, we can also select elements using conditions, like so:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "zeros = X_digits[y_digits==0]  # select all the rows of X where y is seven (i.e. the sevens)\n",
      "ones = X_digits[y_digits==1]   # select all the rows of X where y is nine (i.e. the nines)\n",
      "print \"zeros.shape: \", zeros.shape\n",
      "print \"ones.shape: \", ones.shape"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Lets have a quick look to see that we did it right."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "plt.matshow(ones[0].reshape(28, 28))  # change the 0 to another number to see some more nines. Or try looking at some sevens."
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now, we generate some new data set.\n",
      "\n",
      "For the training labels y, we write as many 0's as the ``zeros`` array is long, the asame for ``ones``."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "X_new = np.vstack([zeros, ones])  # this \"stacks\" sevens and nines vertically\n",
      "print \"X_new.shape: \", X_new.shape\n",
      "y_new = np.hstack([np.repeat(0, zeros.shape[0]), np.repeat(1, ones.shape[0])])\n",
      "print \"y_new.shape: \", y_new.shape\n",
      "print \"y_new: \", y_new"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now we shuffle them around and create a training and test dataset."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.utils import shuffle\n",
      "X_new, y_new = shuffle(X_new, y_new)\n",
      "X_mnist_train = X_new[:5000]\n",
      "y_mnist_train = y_new[:5000]\n",
      "X_mnist_test = X_new[5000:]\n",
      "y_mnist_test = y_new[5000:]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now let us learn a logistic regression model."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "logreg.fit(X_mnist_train, y_mnist_train)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "... and visualize the coefficients.\n",
      "\n",
      "We can see that the middle is dark, corresponding to  high positive values for where\n",
      "a 1 would be. Around it is a lighter circle, corresponding to the position of a 0."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "plt.matshow(logreg.coef_.reshape(28, 28))\n",
      "plt.colorbar()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Finally, let as look at the accuracy on training and test set:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print \"Accuracy training set:\", logreg.score(X_mnist_train, y_mnist_train)\n",
      "print \"Accuracy test set:\", logreg.score(X_mnist_test, y_mnist_test)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Playing around with this notebook\n",
      "=================================\n",
      "1. What would be the accuracy of a completely random predictor on the first dataset? How can you simulate that using Numpy?\n",
      "\n",
      "2. Can you still pick out the class by hand if you set the ``cluster_std`` to ``10`` in ``generate_lobs``? Can logistic regression do it?\n",
      "\n",
      "3. Try to separate some other digit classes from MNIST. Which are hard, which are not?\n",
      "\n",
      "4. Visualize the classes 0 and 1 using PCA. Would you expect they can be separated with a linear classifier?"
     ]
    }
   ],
   "metadata": {}
  }
 ]
}