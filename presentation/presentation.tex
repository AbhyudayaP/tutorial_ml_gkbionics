% rubber: module xelatex
\documentclass[english,final,compress]{beamer}
%\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{fixltx2e}
\usepackage{graphicx}
\usepackage{longtable}
\usepackage{float}
\usepackage{wrapfig}
\usepackage{soul}
\usepackage{textcomp}
\usepackage{marvosym}
\usepackage{wasysym}
\usepackage{latexsym}
\usepackage{amssymb}
\usepackage{hyperref}
\tolerance=1000
\mode<beamer>{\usetheme{Bonn}}
\usepackage{fontspec}
\defaultfontfeatures{Mapping=tex-text}
\usepackage{xunicode}
\usepackage{xltxtra}
\setmainfont[Scale=0.86,Mapping=tex-text]{News Gothic MT}
\setsansfont[Scale=0.86,Mapping=tex-text]{News Gothic MT}
\setmonofont[Scale=0.86,Mapping=tex-text]{Andale Mono}
\usepackage{polyglossia}
\setdefaultlanguage{english}
\usepackage[EU1]{fontenc}
\usepackage{listings}
\usepackage{color}
\usepackage{csquotes}
\usepackage{mdwtab}
\usepackage{booktabs}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amstext}
\usepackage{amsthm}
\renewcommand\maketitle{\frame[plain]{\titlepage}\addtocounter{framenumber}{-1}}
\providecommand{\alert}[1]{\textbf{#1}}

\title{Tutorial Machine Learning in Python}
\author{Andreas M\"{u}ller, Hannes Schulz, and Sven Behnke\\[5mm]\includegraphics[width=.2\linewidth]{style/Logo_UBo_h24_4c-crop}}
\date{ICANN 2012}
\begin{document}

\maketitle


\lstset{%
    basicstyle=\small\tt,
    keywordstyle=\color{beamer@bonnblue}\bfseries, % style for keywords
    numbers=none, % where to put the line-numbers
    numberstyle=\tiny, % the size of the fonts that are used for the line-numbers
    showspaces=false, % show spaces adding particular underscores
    showstringspaces=false, % underline spaces within strings
    showtabs=false, % show tabs within strings adding particular underscores
    tabsize=2, % sets default tabsize to 2 spaces
    captionpos=b, % sets the caption-position to bottom
    breaklines=true, % sets automatic line breaking
    breakatwhitespace=false, 
}
\newcommand\fenc{f_{\mathrm{enc}}}
\newcommand\fdec{f_{\mathrm{dec}}}
\newcommand\Wand{\ensuremath{W_{\mathbf{and}}}}
\newcommand\Wor{\ensuremath{W_{\mathbf{or}}}}
\newcommand{\w}[1]{\ensuremath{\mathbf{#1}}}
\newcommand\loss{\ell}

\frame[plain]{\frametitle{Outline}\tableofcontents\addtocounter{framenumber}{-1}}
\section{Introduction}

\begin{frame}
    \frametitle{Machine Learning Overview}
    \begin{itemize}
        \item Intro: Andy/Hannes?
        \item Before Lunch: Unsupervised Learning (Hannes)
        \item After Lunch: Supervised Learning (Nenad LinReg, Andy LogReg/kNN)
    \end{itemize}
\end{frame}

\section{Unsupervised Learning}

\subsection{PCA}

\begin{frame}
    \frametitle{PCA -- Theory}
    \begin{itemize}
        \item  have: high-dim noisy data, no way to look at it
        \item  Dimensionality Reduction (e.g.\ for visualization)
        \item  Determining relevant axes
        \item  Noise removal
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{PCA -- Interactive}
    \begin{itemize}
        \item  load data
        \item  plot data
        \item  do PCA on blobs
        \item  plot transformed/backtransformed data
        \item  select subset of eigenvectors
        \item  plot transformed/backtransformed data
        \item  same with some high-dim bio data (TODO: find good data!)
    \end{itemize}
\end{frame}

\subsection{k-Means}

\begin{frame}
    \frametitle{k-Means -- Theory}
\end{frame}

\begin{frame}
    \frametitle{k-Means -- Interactive}
    \begin{itemize}
        \item  find blobs on artificial data
        \item  color discretization in an image (TODO: bio images (--> Kristian and Baukhage?))
        \item  visualize cluster centers in "interesting data" (TODO: data??)
    \end{itemize}
\end{frame}

\section{Supervised Learning}


\begin{frame}
    \frametitle{Supervised Learning -- General}
    \begin{itemize}
        \item Supervised Learning
        \item Training, Testing
    \end{itemize}
\end{frame}

\subsection{Linear Regression}

\begin{frame}
    \frametitle{Linear Regression -- Theory}
    \begin{itemize}
        \item talk about regression coefficients
        \item  prediction error
        \item  draw line in 2D
        \item  (draw plane in 3D)
        \item  formula for hyperplane
        \item  formula for the loss
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Linear Regression -- Interactive}
    \begin{itemize}
        \item  use some real-world dataset
            http://people.sc.fsu.edu/~jburkardt/datasets/regression/regression.html
        \item  find w hat by calling linear regression function
        \item  measure prediction error for withheld subset data
        \item  plot result (tricky)
        \item  same using boston housing or diabetes dataset
    \end{itemize}
\end{frame}

\subsection{Logistic Regression}

\begin{frame}
    \frametitle{Logistic Regression -- Theory}
    \begin{itemize}
        \item have: data, binary categorical variable
        \item  want: simple (linear, interpretable?) function that distinguishes between classes 
        \item  how:
        \item  draw line in 2D on blobs
        \item  (draw plane in 3D) on blobs
        \item  formula for hyperplane
        \item  minimize error using
        \item  formula for the loss
    \end{itemize}
\end{frame}

\subsection{$k$ Nearest Neighbor}

\begin{frame}
    \frametitle{Logistic Regression -- Interactive}
    \begin{itemize}
        \item some 2D blobs dataset
        \item  Breast Cancer dataset
        \item  http://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+%28Diagnostic%29
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{$k$ Nearest Neighbor -- Theory}
    \begin{itemize}
        \item Slides:
        \item  have: data, categorical variable
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{$k$ Nearest Neighbor -- Interactive}
    \begin{itemize}
        \item  start with blobs from k-means
        \item  Iris dataset
        \item  Measure prediction error
        \item  Maybe: Complexity curve visualization (needs a loop)
    \end{itemize}
\end{frame}

\end{document}
